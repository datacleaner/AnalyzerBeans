AnalyzerBeans architecture

 There are several issues to cover when you want to understand the architecture of AnalyzerBeans. On this page we will collect the high-level concepts that make up the framework in AnalyzerBeans.

Analyzers, Transformers and Filters

 There are a few archetypical component types in AnalyzerBeans:
 
  * Transformers: These components are used to take some input from a row and create new content for that same row (in the form of a new set of columns). The original values will always be retained but they can be ignored if only the generated values are of interest. Examples of transformers include components that convert data types, extract certain information, tokenize, combine and standardize values.
  * Filters: Filters are used to categorize rows with the intention of splitting the processing flow so that the user can choose to only process a subset of rows or apply different subflows for different types of rows.
  * Analyzers: Analyzers are the endpoints in a data processing flow. Analyzers consume all rows in a (sub)flow and once all rows have been consumed, they generate a result. An analyzer's result will typically be rendered to the format that is appropriate for the current session (ie. as text if you're running a command line application, as a Swing component if you're running in a Swing environment or as HTML if you're running in a web environment).

 The component types are displayed in the figure below:

[component-types.png]

Configuration and jobs

 The user of AnalyzerBeans needs to supply two things to make the framework execute: A configuration and an analysis job.
 
 The configuration is an object containing information about the environment of the system. This includes a catalog of datastores, a catalog of reference data and so on.
 
 An analysis job contain instructions for processing and analyzing data in a specified environment (ie. using a configuration). The job consists of a flow of source columns, transformers, filters, analyzers and so on. Each of these are used to compute the optimal execution flow.
 
 Some points about analysis jobs:
 
  * The java-class AnalysisJob is the main entry-point for a job. The job has a set of children job-types for each component added: AnalyzerJob, TransformerJob, FilterJob etc.

  * Both the configuration and the analysis job are immutable. This means that they cannot change state. If you want to change your job, then you actually create a new job. This is useful since AnalyzerBeans is a highly concurrent application and when objects are immutable there's no risk of race-conditions even if several threads access these objects.

  * If you want to programmatically build or modify an analysis job, then you should use the AnalysisJobBuilder class. It contains a lot of helper methods and extra utilities to observe, traverse and quickly build a job.

Dictionaries, synonyms and other reference data

 TODO
 
Row processing framework

 TODO hello
 
Exploring analyzers

 TODO

Row annotations

 TODO
 
Concurrency model

 AnalyzerBeans is designed to be executable in a highly concurrent environement, while retaining the option to run it in as a regular single threaded application. This is archieved by a pluggable execution layer called the TaskRunner. There are three built-in task runner implementations (and new implementations are quite easy to add): Single threaded task runner, Multithreaded task runner (uses the java.util.concurrent stack) and EJB Timer task runner (for deployment in an EJB 3.0+ container).
 
 The concurrency model is based on a strategy where a rather fine level of granularity of tasks is archieved in order to fully utilize multithreading (ie. finer granularity means better distribution of tasks to threads). Here are the main tasks that make up the execution flow:

  * All components in an analysis job is being initialized and closed down in separate tasks.

  * When running an analysis job in AnalyzerBeans each processed table will be queried in it's own task.

  * Most importantly each row of a datastore will be processed in it's own task. This means that components usually don't block each other (as they would in the opposite case where each component had it's own thread) and that the amount of rows being processed concurrently will often time be equal to the amount of available threads. In other words: We utilize the threads as much as possible. It also prevents the system from running out of memory because we will never get into a case where one component is consuming rows faster than another component, which would require a "blocking buffer" strategy to keep down memory consumption.
 
 A few rules pertain to the concurrent processing:
 
  * Transformers and Filters are assumed to run without race conditions in a concurrent manner. If you need to change this behaviour for a particular Transformer or Filter there are two ways: Either use Java's synchronized keyword or annotate your class with AnalyzerBeans' annotation for this purpose: @Concurrent(false).
  
  * Analyzers are assumed to have race conditions because normally they will be used to build up results from the stream of rows that it consumes. Therefore the default concurrent behaviour of an analyzer is that only a single thread at a time is allowed to invoke it's run(...) method. To override this behaviour use AnalyzerBeans' annotation for this purpose: @Concurrent(true).